"""
æ–‡æ¡£æŸ¥è¯¢æ¨¡å— - RAGç³»ç»Ÿwith DeepSeekå¢žå¼ºå›žç­”

è¯¥æ¨¡å—æä¾›RAGæ£€ç´¢åŠŸèƒ½ï¼Œä½¿ç”¨deepseekè¿›è¡Œå¢žå¼ºå›žç­”
å‚è€ƒsix_sister_query.pyçš„å®žçŽ°æ–¹å¼
"""

import os

# åœ¨å¯¼å…¥ä»»ä½•å…¶ä»–æ¨¡å—ä¹‹å‰è®¾ç½®çŽ¯å¢ƒå˜é‡ä»¥é¿å…ONNX Runtimeè­¦å‘Š
os.environ["ORT_DISABLE_TENSORRT"] = "1"
os.environ["ONNXRUNTIME_EXECUTION_PROVIDERS"] = "CPUExecutionProvider"

from langchain_core.documents.base import Document
from typing import List
from langchain_community.vectorstores import Chroma
from langchain_core.stores import InMemoryStore
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from langchain_classic.retrievers.multi_vector import MultiVectorRetriever
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from mongo import get_mongo_doc_store
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

from langchain_classic.retrievers import MultiQueryRetriever  # ç”¨äºŽå¤šæŸ¥è¯¢æ£€ç´¢å™¨ï¼Œå¯ä»¥æŠŠä¸€ä¸ªé—®é¢˜æ”¹å†™æˆå¤šä¸ªé—®é¢˜
#from langchain_classic.retrievers import EnsembleRetriever  # ç”¨äºŽBM25å…³é”®è¯æ£€ç´¢å™¨
#from langchain_community.retrievers import ContextualCompressionRetriever
from langchain_community.retrievers import BM25Retriever
from langchain_community.retrievers import AmazonKendraRetriever
#from langchain_community.retrievers import EnsembleRetriever

class DocumentQuery:
    """æ–‡æ¡£æŸ¥è¯¢ç±»ï¼Œæä¾›RAGæ£€ç´¢åŠŸèƒ½with DeepSeekå¢žå¼ºå›žç­”"""
    
    def __init__(self, 
                 vectorstore_path: str = "./chroma_db",
                 collection_name: str = "transformers",
                 mongo_db_name: str = "transformers",
                 mongo_collection_name: str = "documents",
                 embedding_model_path: str = None,
                 deepseek_api_key: str = None):
        """
        åˆå§‹åŒ–æŸ¥è¯¢ç³»ç»Ÿ
        
        Args:
            vectorstore_path: Chromaå‘é‡æ•°æ®åº“è·¯å¾„
            collection_name: Chromaé›†åˆåç§°
            mongo_db_name: MongoDBæ•°æ®åº“åç§°
            mongo_collection_name: MongoDBé›†åˆåç§°
            embedding_model_path: åµŒå…¥æ¨¡åž‹è·¯å¾„
            deepseek_api_key: DeepSeek APIå¯†é’¥
        """
        self.vectorstore_path = vectorstore_path
        self.collection_name = collection_name
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡åž‹ï¼ˆä½¿ç”¨åŒè¯­åµŒå…¥æ¨¡åž‹ï¼‰
        if embedding_model_path is None:
            embedding_model_path = r"C:\Users\Zhi-F\.cache\modelscope\hub\models\BAAI\bge-base-en-v15"
        reranker_model_path = r"C:\Users\Zhi-F\.cache\modelscope\hub\models\BAAI\bge-reranker-base"
  
 
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model_path
        )

        print(f"Loading Reranker model from: {reranker_model_path}")
        self.reranker = HuggingFaceCrossEncoder(model_name=reranker_model_path)

        
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vectorstore = Chroma(
            collection_name=collection_name,
            embedding_function=self.embeddings,
            persist_directory=vectorstore_path
        )
        
        # åˆå§‹åŒ–æ–‡æ¡£å­˜å‚¨ (ä½¿ç”¨é»˜è®¤é…ç½®ï¼šæ•°æ®åº“"transformers"ï¼Œé›†åˆ"documents")
        self.doc_store = get_mongo_doc_store()
        
        # åˆ›å»ºæ£€ç´¢å™¨ï¼ˆæ·»åŠ æ›´å¤šæŸ¥è¯¢å‚æ•°ä¼˜åŒ–ï¼‰
        self.retriever = MultiVectorRetriever(
            vectorstore=self.vectorstore,
            docstore=self.doc_store,
            id_key="doc_id",
            search_type="similarity",
            # å¯ä»¥æ·»åŠ ä»¥ä¸‹å‚æ•°æ¥ä¼˜åŒ–æŸ¥è¯¢ï¼š
             search_kwargs={
                 "k": 8,  # è¿”å›žæ–‡æ¡£æ•°é‡ï¼ˆé»˜è®¤4ï¼‰
             }
        )
 
        
        # åˆå§‹åŒ–DeepSeek LLM
        # Prefer explicit parameter, otherwise read from environment variable
        if deepseek_api_key is None:
            deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")

        if not deepseek_api_key:
            raise RuntimeError("DEEPSEEK_API_KEY is not set. Please set the DEEPSEEK_API_KEY environment variable.")

        os.environ["DEEPSEEK_API_KEY"] = deepseek_api_key
        self.llm_deepseek = ChatOpenAI(
            model_name="deepseek-chat",
            openai_api_base="https://api.deepseek.com/v1",
            openai_api_key=os.environ["DEEPSEEK_API_KEY"],
            temperature=0.3,
            request_timeout=60,
            max_retries=1,
            callbacks=[],
        )

        # Prompt template (English text, instructs assistant to answer in Chinese)
        self.prompt = ChatPromptTemplate.from_template("""
            You are a professional document analysis assistant. Please answer the user's question based on the provided context.

            Please follow these requirements:
            1. Answer in English.
            2. Be accurate, complete, and well-structured.
            3. If the question involves specific technical concepts, provide detailed explanations.
            4. If the context is insufficient or the information needed is not present in the provided context, explicitly say "I cannot answer based on the provided context." Do not attempt to hallucinate or invent facts.
            5. Do not mention images or charts; answer based on the textual content only.

            Context:
            {context}

            Question: {question}

            Please provide a detailed answer or explicitly state you cannot answer.
            """)
 
    
    def query(self, query_text: str, top_k: int = None) -> List[Document]:
        """
        æŸ¥è¯¢æ–‡æ¡£ï¼ˆåŸºç¡€æ£€ç´¢ï¼‰- ä¿®å¤ç‰ˆæœ¬
        
        çœŸæ­£çš„ä¸¤é˜¶æ®µæŸ¥è¯¢ï¼š
        1. å‘é‡æ•°æ®åº“æœç´¢ç›¸ä¼¼å‘é‡
        2. æ ¹æ®doc_idä»ŽMongoDBèŽ·å–å®Œæ•´æ–‡æ¡£
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›žç»“æžœæ•°é‡é™åˆ¶
            
        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        
        try:
            print(f"ðŸ” å¼€å§‹ä¸¤é˜¶æ®µæŸ¥è¯¢: '{query_text}'")
            
            # ç¬¬ä¸€é˜¶æ®µï¼šå‘é‡æ•°æ®åº“æœç´¢
            print("ðŸ“‹ é˜¶æ®µ1: å‘é‡æ•°æ®åº“æœç´¢...")
            if top_k:
                vector_results = self.retriever.vectorstore.similarity_search(query_text, k=top_k)
            else:
                vector_results = self.retriever.vectorstore.similarity_search(query_text, k=8)
            
            print(f"âœ… å‘é‡æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(vector_results)} ä¸ªå‘é‡ç»“æžœ")
            
            # æå–doc_id
            doc_ids = []
            vector_info = []
            
            for i, doc in enumerate(vector_results):
                doc_id = doc.metadata.get('doc_id')
                if doc_id:
                    doc_ids.append(doc_id)
                    vector_info.append({
                        'index': i,
                        'doc_id': doc_id,
                        'content_preview': doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
                    })
                    print(f"  ðŸ“‹ æå–doc_id: {doc_id}")
                else:
                    print(f"  âš ï¸  å‘é‡ç»“æžœ {i} ç¼ºå°‘doc_id")
            
            if not doc_ids:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•doc_id")
                return []
            
            # ç¬¬äºŒé˜¶æ®µï¼šä»ŽdocstoreèŽ·å–å®Œæ•´æ–‡æ¡£
            print(f"ðŸ“‹ é˜¶æ®µ2: ä»ŽMongoDBèŽ·å–å®Œæ•´æ–‡æ¡£...")
            print(f"  ðŸ” æŸ¥è¯¢ {len(doc_ids)} ä¸ªdoc_id: {doc_ids}")
            
            full_docs = self.doc_store.mget(doc_ids)
            print(f"âœ… MongoDBæŸ¥è¯¢å®Œæˆï¼ŒèŽ·å¾— {len(full_docs)} ä¸ªå®Œæ•´æ–‡æ¡£")
            
            # åˆå¹¶vectoræœç´¢ç»“æžœå’Œå®Œæ•´æ–‡æ¡£
            docs = []
            for i, (vector_doc, full_doc) in enumerate(zip(vector_results, full_docs)):
                doc_id = vector_info[i]['doc_id']
                
                if full_doc:
                    # ä¼˜å…ˆä½¿ç”¨docstoreä¸­çš„å®Œæ•´æ–‡æ¡£
                    if isinstance(full_doc, dict):
                        # ç›´æŽ¥ä½¿ç”¨MongoDBè¿”å›žçš„å®Œæ•´ç»“æž„
                        metadata = full_doc.get('metadata', {}) or {}
                        
                        # ç¡®ä¿åŒ…å«doc_typeä¿¡æ¯ï¼ˆæ¥è‡ªMongoDBçš„_deserialize_documentæ–¹æ³•ï¼‰
                        doc_type = full_doc.get('doc_type', 'unknown')
                        if doc_type != 'unknown':
                            metadata['doc_type'] = doc_type
                        
                        # å¦‚æžœMongoDBæœ‰è¿”å›žå®Œæ•´çš„doc_dataç»“æž„ï¼Œç›´æŽ¥ä½¿ç”¨å®ƒ
                        raw_doc_data = full_doc.get('raw_doc_data', {})
                        if raw_doc_data:
                            metadata['doc_data'] = raw_doc_data
                        
                        # æå–å›¾ç‰‡æ•°æ®
                        if 'image_base64' in full_doc:
                            metadata['image_base64'] = full_doc['image_base64']
                            metadata['content_type'] = full_doc.get('content_type', 'image/png')
                            metadata['original_filename'] = full_doc.get('original_filename', 'image.png')
                        
                        # åˆ›å»ºDocumentå¯¹è±¡
                        doc = Document(
                            page_content=full_doc.get('page_content', ''),
                            metadata=metadata
                        )
                        # ç¡®ä¿åŒ…å«doc_id
                        doc.metadata['doc_id'] = doc_id
                        
                        print(f"  âœ… MongoDBå®Œæ•´æ–‡æ¡£ {doc_id}: {len(doc.page_content)} å­—ç¬¦")
                        print(f"      doc_type: {doc_type}")
                        print(f"      åŒ…å«å›¾ç‰‡: {'æ˜¯' if 'image_base64' in metadata else 'å¦'}")
                    elif isinstance(full_doc, Document):
                        # å¦‚æžœå·²ç»æ˜¯Documentå¯¹è±¡
                        doc = full_doc
                        if 'doc_id' not in doc.metadata:
                            doc.metadata['doc_id'] = doc_id
                        print(f"  âœ… MongoDB Documentå¯¹è±¡ {doc_id}: {len(doc.page_content)} å­—ç¬¦")
                    else:
                        # å…œåº•ï¼šä½¿ç”¨vectoræœç´¢ç»“æžœ
                        doc = vector_doc
                        print(f"  âš ï¸  ä½¿ç”¨å‘é‡æœç´¢ç»“æžœ {doc_id}: {len(doc.page_content)} å­—ç¬¦")
                else:
                    # å…œåº•ï¼šä½¿ç”¨vectoræœç´¢ç»“æžœ
                    doc = vector_doc
                    print(f"  âš ï¸  MongoDBæœªæ‰¾åˆ°ï¼Œä½¿ç”¨å‘é‡æœç´¢ç»“æžœ {doc_id}: {len(doc.page_content)} å­—ç¬¦")
                
                # æå–å›¾ç‰‡æ•°æ®
                if hasattr(doc, 'metadata') and doc.metadata:
                    image_data = {}
                    if 'image_base64' in doc.metadata:
                        image_data['image_base64'] = doc.metadata['image_base64']
                        image_data['content_type'] = doc.metadata.get('content_type', 'image/png')
                        image_data['original_filename'] = doc.metadata.get('original_filename', f'image_{i+1}.png')
                    
                    if image_data:
                        print(f"  ðŸ–¼ï¸  {doc_id} åŒ…å«å›¾ç‰‡æ•°æ®: {list(image_data.keys())}")
                
                # æå–è¡¨æ ¼HTMLæ•°æ®ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
                if hasattr(doc, 'metadata') and doc.metadata:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼çš„HTMLè¡¨ç¤º
                    if 'text_as_html' in doc.metadata:
                        # å°†è¡¨æ ¼HTMLæ•°æ®æ·»åŠ åˆ°metadataä¸­ï¼Œä¾›å‰ç«¯ä½¿ç”¨
                        doc.metadata['table_html'] = doc.metadata['text_as_html']
                        print(f"  ðŸ“Š  {doc_id} åŒ…å«è¡¨æ ¼æ•°æ®")
                    elif hasattr(doc, 'text_as_html'):
                        # å¦‚æžœtext_as_htmlæ˜¯æ–‡æ¡£å¯¹è±¡çš„å±žæ€§
                        doc.metadata['table_html'] = doc.text_as_html
                        print(f"  ðŸ“Š  {doc_id} åŒ…å«è¡¨æ ¼æ•°æ®")

                docs.append(doc)
            
            print(f"âœ… ä¸¤é˜¶æ®µæŸ¥è¯¢å®Œæˆï¼Œè¿”å›ž {len(docs)} ä¸ªå®Œæ•´æ–‡æ¡£")
            return docs
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []


    def generate_answer(self, query_text: str, docs: list[Document], answer_top_k: int = 3) -> tuple[str, list[dict]]:
        """
        Generate an enhanced answer using DeepSeek (or configured llm).
        This is a new method to avoid changing existing `query()` behaviour.

        Returns: (answer_text, sources)
        - answer_text: the generated answer string, or a safe refusal string when evidence insufficient
        - sources: list of dicts with keys `id`, `page`, `title`, `source` (mirrors existing response shape)
        """
        # basic gating: if no docs, refuse
        if not docs:
            return (
                "I cannot answer based on the provided documents.",
                [],
            )

        # pick top-k documents (preserve order if already ranked)
        selected = docs[:answer_top_k]

        # Build context snippets
        ctx_parts: list[str] = []
        sources: list[dict] = []
        for d in selected:
            content = (d.page_content or "").strip()
            # truncate to avoid huge prompts
            snippet = content[:1500]
            ctx_parts.append(snippet)
            sources.append({
                "id": d.metadata.get("id") or d.metadata.get("source"),
                "page": d.metadata.get("page"),
                "title": d.metadata.get("title"),
                "source": d.metadata.get("source"),
            })

        context_text = "\n\n---\n\n".join(ctx_parts)

        # Construct prompt: use existing prompt template if present, else a simple template
        if hasattr(self, "prompt") and isinstance(self.prompt, ChatPromptTemplate):
            # try to use the template's formatting; append the context
            try:
                prompt_text = (
                    self.prompt.format_messages({"query": query_text})[0].content
                    + "\n\nContext:\n"
                    + context_text
                    + "\n\nIf the provided documents do not contain enough information to answer, reply exactly: 'I cannot answer based on the provided documents.'"
                )
            except Exception:
                prompt_text = (
                    f"Question: {query_text}\n\nContext:\n{context_text}\n\nIf the provided documents do not contain enough information to answer, reply exactly: 'I cannot answer based on the provided documents.'"
                )
        else:
            prompt_text = (
                f"Answer the question based only on the following extracted document snippets. If you cannot answer from these snippets, reply exactly: 'I cannot answer based on the provided documents.'\n\nQuestion: {query_text}\n\nContext:\n{context_text}"
            )

        # Attempt to call the configured DeepSeek LLM (self.llm_deepseek) if available
        answer_text = None
        try:
            if hasattr(self, "llm_deepseek") and self.llm_deepseek is not None:
                # ChatOpenAI typically supports __call__ or predict
                try:
                    # prefer simple call
                    answer_text = self.llm_deepseek.predict(prompt_text)
                except Exception:
                    try:
                        resp = self.llm_deepseek.generate([prompt_text])
                        # attempt to extract text
                        if resp and hasattr(resp, "generations") and len(resp.generations) > 0:
                            answer_text = resp.generations[0][0].text
                    except Exception:
                        answer_text = None
            else:
                if hasattr(self, "llm") and self.llm is not None:
                    try:
                        answer_text = self.llm.predict(prompt_text)
                    except Exception:
                        answer_text = None
        except Exception:
            answer_text = None

        # fallback: if still no answer_text, produce a safe refusal
        if not answer_text:
            return (
                "I cannot answer based on the provided documents.",
                sources,
            )

        # Return answer and provenance
        return (answer_text, sources)